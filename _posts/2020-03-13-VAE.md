<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
---
title: '变分自编码研究'
<!-- tags:
  - Reading notes
  - Variational Autoencoder
---
This is my understanding of VAEs
------

<!-- TOC -->

## 生成模型
<center>
    <img style="border-radius: 0.2525em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 6px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/Kaimaoge/Kaimaoge.github.io/master/images/generative.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图１：生成模型</div>
</center>

VAE是目前最为火热的非监督学习研究方向，这篇博文是本人对变分自编码的阅读和相关理解。变分自编码是一种生成模型。如图１所示，对于生成模型，我们的目标在于利用一些潜在的变量$z$去生成真实的数据$X$。比如说我们想要生成０到９的图像，这个潜在变量$z$可以是0到9的数字。这个生成的过程可以用一个映射$f(z；\theta)$来表示。$z$是潜在变量，而$\theta$是决定映射关系的参数。用概率来表示，我们有

\begin{equation}
P(X)=\int P(X | z;\theta)P(z)dz
\end{equation}

一个生成模型有两个问题需要解决，一是如何建模$z$，$z$应该是什么样的分布，另一个就是如何去确定$f(；\theta)$。确定$f(；\theta)$并不容易，因为牵涉到了对$z$的积分。在最基础的VAE之中，$z$被建模为标准分布$N(0,I)$。而$f(z；\theta)$被构建成了一个深度的神经网络，神经网络的多层表征可以将$z$映射到具有语义特性的表达上。如果$z \sim N(0,I)$，那么

\begin{equation}
P(X | z;\theta) \sim N(X | f(z;\theta), \sigma^2 \ast I)
\end{equation}

在深度学习研究中，如果我们可以找到$P(X)$的数学表达式，就可以利用梯度下降对其求解。一种方式是去采样$n$个$(z_i,z_2,\cdots,z_n)$，然后

\begin{equation}
P(X) = \frac{1}{n} P(X_i | z_i)
\end{equation}

## 求解方法

这一方式对于复杂的数据集并不可行，我们需要采样大量的$z$才能对数据进行准确的估计。换一种思路，如果我们可以快速的采样到最有可能产生X的潜在变量z，那么我们就可以快速的用少量的潜在变量z来生成X，并估计出X的概率。那么对于给定数据X，我们可以采样出最可能产生X的z吗？在VAE中，这个采样的概率被表达为Q(z)，而VAE的损失函数则可从Q(z)和$P(z\|X)$之间的KL散度推导得到。

\begin{equation}
KL(Q(z) || P(z|X))　= E_{z \sim Q}(logQ(z) - logP(z|X))
\end{equation}

利用Beyes定理，有

\begin{equation}
= E_{z \sim Q}(logQ(z) - logP(X|z) - logP(z)) + logP(x)
\end{equation}

\begin{equation}
logP(x) - KL(Q(z) || P(z|X)) = E_{z \sim Q}(logP(X|z)) - KL(Q(z) || P(z))
\end{equation}

我们可以以任意方式建模Q(z)。在这里，因为我们希望根据X就能采样出最有可能产生Ｘ的z，我们将Q(z)建模为由X决定的分布$Q(z\|X)$，那么等式变为以下公式

\begin{equation}
logP(x) - KL(Q(z\|X) || P(z|X)) = E_{z \sim Q}(logP(X\|z)) - KL(Q(z\|x) || P(z))
\end{equation}

通过优化以上等式左侧的表达式，我们可以最大化logP(x)，即最大化生成数据的可能性；另外最小化了$Q(z\|X)$和$P(z\|X)$的KL散度，即$Q(z\| X)$和$P(z\|X)$尽可能的接近了。而等式右侧的部分可以通过梯度下降求解。而对应于自编码，$Q(z\|x)$是一个编码器，而$P(X\|z)$是一个解码器。将$Q(z\|x)$建模为高斯分布，其期望和方差由神经网络输出。我们有$Q(z\| X) = N(z \| \mu(X; \theta), \Sigma(X; \theta))$，其中$\theta$是神经网络的参数。已知我们假设$z$服从$N(0,1)$分布，那么$KL(Q(z\|x) \|\| P(z))$等于

\begin{equation}
KL(N( \mu(X), \Sigma(X) || N(0, 1)) = \frac{1}{2} (Tr(\Sigma(X)) + \mu(X)^T \mu(X) - k - log det(\Sigma(X))),
\end{equation}

另外换一个角度来看，$logP(x)$与$Q(z \| x)$无关，最小化$E_{z \sim Q}(logP(X|z)) - KL(Q(z\|x) || P(z))$等价于最小化$KL(Q(z\|X) \|\| P(z\|X))$。所以$E_{z \sim Q}(logP(X\|z)) - KL(Q(z\|x) \|\| P(z))$又被认为是 ELBO（Evidence lower bound）。即最大化ELBO，等价于最小化
假设分布和后验分布之间的距离。很多VAE的变种都是去寻找其它对z分布假设下的ELBO而进行求解的。

## 再参数化

我们可以利用上式来梯度下降求解$KL(Q(z\|x) \|\| P(z))$，但是求解编码器$E_{z \sim Q}(logP(X\|z))$并不容易，因为$Q(z\|x)$是一个分布，梯度下降无法处理随机的元素。为了使得编码器也可解，VAE作者使用了再参数化Trick。从$N( \mu(X), \Sigma(X)$采样可以先采样$\eta \sim N(0,1)$，然后$z = \mu(X) + \Sigma^{1/2}(X)\eta$。这样解码器的梯度也变得可解了。

\begin{equation}
E_{z \sim Q}(logP(X|z))　＝　E_{\eta \sim N(0,1)}(logP(X|z = \mu(X) + \Sigma^{1/2}(X)\eta)),
\end{equation}

我们还可以把$Q(z\|x)$建模为其它的分布，下图附上了各种分布的再参数化Trick。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 8px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/Kaimaoge/Kaimaoge.github.io/master/images/rp.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图2：[各种分布的在参数化Trick](http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/)</div>
</center>

## VAE的理解

VAE和解绑表征有着奇妙的联系，因为假设$z \sim N(0,I)$，$z$中每一个元素都是独立的，也就是说我们变换$z$中任何一个元素都会对应$X$变化的某一个属性，正是基于这一观察，一些学者提出了$\beta$-VAE。$\beta$-VAE对损失函数进行了更改:

\begin{equation}
E_{z \sim Q}(logP(X\|z)) - \beta KL(Q(z\|x) || P(z))
\end{equation}

其中$\beta > 1$，这一模型更为重视$Q(z\|x)$和$N(0,I)$的距离，即让编码器生成的$z$更加近似于独立多元高斯分布。这样更容易找到解绑的表达$z$。

**Disentangled representation is an unsupervised learning technique that breaks down, or disentangles, each feature into narrowly defined variables and encodes them as separate dimensions.**


To be continued





