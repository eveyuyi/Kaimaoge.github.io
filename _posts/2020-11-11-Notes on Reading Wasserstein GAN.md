
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> 

## Essence of Generative Adversarial Networks (GAN)

GAN belongs to unsurpevised learning. Its purpose is to learn a probability density from data. In tradition, a probability density is learnt by defining a probability function $P_\theta$ defined by parameter set $\theta$ and finding $\theta^\star$ maximizing the likelihood on our data $\{ x^{(i)}\}^m_{i=1}$. To achieve that, we solve

\begin{equation}
\max_\theta \frac{1}{m} \sum^m_{i = 1} log(P_\theta({x^{(i)}}))
\end{equation}

Assuming the real density for data $\{ x^{(i)}\}^m_{i=1}$ is $P_r$, the optimal $P_{\theta^\star}$ is equal to $P_r$ everywhere. It will minimize the Kullback-Leibler divergence $KL(P_r \parallel P_{\theta^\star})$.

\begin{equation}
KL(P_r \parallel P_{\theta}) = \int_x p_r(x) \log \frac{p_r(x)}{p_\theta(x)} dx
\end{equation}

Instead of directly finding $P_{\theta^\star}$, GAN uses a neural network $g_{\theta}$ with parameter $\theta$ that directly
generates samples following a certain distribution $P_{\theta}$. The input of $g_{\theta}$ is a random variable $Z$ with a fixed distribution $p(z)$. Through this approach, we can easily generate samples from a given distribution $p(z)$. $P_{r}$ can be represented by a low dimensional manifold. $z$ would have some semantic features.

<center>
    <img style="border-radius: 0.13em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 6px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/Kaimaoge/Kaimaoge.github.io/master/images/generator.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Figure １：DCGAN generator used in reference [1]. This structure projects a 100-dimensional vector obeying normal distribution to a realistic image. </div>
</center>

## Distance of traditional GAN

The GAN model architecture involves two sub-models: a generator model $G()$ for generating new examples and a discriminator model $D()$ for classifying whether generated examples are real, from the domain, or fake, generated by the generator model. 

<center>
    <img style="border-radius: 0.12em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 6px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/Kaimaoge/Kaimaoge.github.io/master/images/GAN.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Figure 2：GAN framework. </div>
</center>

 We train $D()$ to maximize the probability of assigning the correct label (real:1, fake:0) to both training examples and samples from $G()$. This is equal to maximizing $E_{x \sim p_r} (\log D(x)) + E_{z \sim p_z} (\log (1 - D(G(z))))$. $G()$ is trained to make a fool of $D$. This is done by minimizing $E_{z \sim p_z} (\log (1 - D(G(z))))$. In other words,$D$ and $G$ play the following two-player minimax game
 
 \begin{equation}
\min_G \max_D L(G, D) = E_{x \sim p_r} (\log D(x)) + E_{z \sim p_z} (\log (1 - D(G(z)))).
\end{equation}

For fixed $G$, the optimal $D^\star$ is $\frac{p_r}{p_r + p_g}$. Minimizing $L(G, D^\star)$ is equal to minimizing Jensen–Shannon divergence between the $p_g$ and $p_r$ [2]. 

 \begin{equation}
JS(P_r \parallel P_g) = \frac{1}{2} KL(P_r \parallel \frac{P_g + P_r}{2}) + \frac{1}{2} KL(P_g \parallel \frac{P_g + P_r}{2}).
\end{equation}

## Wasserstein GAN

KL and JS distances are measuring how close the model distribution $P_g$ and the real distribution $P_r$ are. The Wasserstein GAN [3] paper points out that the distances' impacts on the convergence of sequences of probability distributions. If the loss function defined upon the distances $d(P_g, P_r)$ is continuous, it is easier for GAN to converge.

[3] finds that Wasserstein distance $W(P_g, P_r)$ is continuous everywhere, and differentiable almost everywhere, if $g$ is continuous in $\theta$ and Lipschitz continuous. 

A real-valued function $f:R \to R$ is called $K$-Lipschitz continuous if there exists a real constant $K \geq 0$ such that, for all $x_1,x_2 \in R$,
\begin{equation}
|f(x_1) - f(x_2)| \leq K|x_1 - x_2|.
\end{equation}
Wasserstein Distance is a measure of the distance between two probability distributions. It can be interpreted as the minimum energy cost of moving and transforming a pile of dirt in the shape of one probability distribution to the shape of the other distribution. Similar to filing holes with piles of dirt. One distribution acts as a group of holes, while the points of the other distribution are dirt.

<center>
    <img style="border-radius: 0.12em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 6px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/Kaimaoge/Kaimaoge.github.io/master/images/wass.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Figure 3：Wasserstein Distance (from https://towardsdatascience.com/earth-movers-distance-68fff0363ef2). </div>
</center>

The Wasserstein Distance $W(P_r, P_g)$ between two continues distribution $P_g$ and $P_r$ is defined by
\begin{equation}
W(P_r, P_g) = inf_{\gamma \sim \amalg(p_r, p_g)} E_{(x,y) \sim \gamma} (\parallel x -y \parallel)
\end{equation}
In the formula above, $\amalg(p_r, p_g)$ is the set of all possible joint probability distributions between $p_r$ and $p_g$.

To show the continues nature of Wasserstein Distance, [3] gives an example of learning parallel lines. In this example, $P_r$ is represented by $(0, Z) \in R^2$ with $Z \sim U(0, 1)$. $G_{\theta}(z) = (\theta, z)$. In other words, $P_g$ is represented by $(\theta, Z)$.
 
<center>
    <img style="border-radius: 0.12em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 6px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/Kaimaoge/Kaimaoge.github.io/master/images/wasserstein_simple_example.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Figure 4：learning parallel lines (from [4]). </div>
</center>

For learning parallel lines, we have
\begin{equation}
KL(P_r, P_g) =   \begin{cases}
    0       & \quad \text{if } \theta = 0\\
    +\infty & otherwise
  \end{cases}
\end{equation}

\begin{equation}
JS(P_r, P_g) =   \begin{cases}
    0       & \quad \text{if } \theta = 0\\
    log2   & otherwise
  \end{cases}
\end{equation}

\begin{equation}
W(P_r, P_g) =  |\theta| 
\end{equation}

Obviously, Wasserstein Distance is the only continues one for this case. It is intractable to exhaust all the possible joint distributions in $\amalg(p_r, p_g)$. Thus the authors proposed a smart transformation of the formula based on the Kantorovich-Rubinstein duality to:
\begin{equation}
W(P_r, P_g) = \frac{1}{K}\sup_{\parallel f_l \parallel \leq K} (E_{x \in p_r} (f(x)) - E_{x \in p_g} (f(x))),
\end{equation}
$\parallel f_l \parallel \leq K$ meaning $f()$ should be K-Lipschitz continuous.

In the Wasserstein-GAN, the “discriminator” model is used to learn $\theta$ to find a good $f_\theta$ and the loss function is the Wasserstein distance between $P_r$ and $P_g$.
 \begin{equation}
L(p_r, p_g) = \max_{\theta \in Theta} (E_{x \in p_r} (f_\theta(x)) - E_{z \in p_z} (f_theta(g(z)))).
\end{equation}

$f_\theta$ is a K-Lipschitz continuous function to help compute Wasserstein distance. To keep it K-Lipschitz continuous during training, Wasserstein GAN clamp the weights $\theta$ to a small window after every gradient update. This preserve the Lipschitz continuity.

## Reference
[1] Radford, Alec, Luke Metz, and Soumith Chintala. "Unsupervised representation learning with deep convolutional generative adversarial networks." arXiv preprint arXiv:1511.06434 (2015).

[2] Goodfellow, Ian, et al. "Generative adversarial nets." Advances in neural information processing systems. 2014. 

[3] Arjovsky, Martin, Soumith Chintala, and Léon Bottou. "Wasserstein gan." arXiv preprint arXiv:1701.07875 (2017).

[4] Weng, Lilian. "From GAN to WGAN." arXiv preprint arXiv:1904.08994 (2019).

