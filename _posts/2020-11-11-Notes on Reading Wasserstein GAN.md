
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> 

## Essence of Generative Adversarial Networks (GAN)

GAN belongs to unsurpevised learning. Its purpose is to learn a probability density from data. In tradition, a probability density is learnt by defining a probability function $P_\theta$ defined by parameter set $\theta$ and finding $\theta^\star$ maximizing the likelihood on our data $\{ x^{(i)}\}^m_{i=1}$. To achieve that, we solve

\begin{equation}
\max_\theta \frac{1}{m} \sum^m_{i = 1} log(P_\theta_{x^{(i)}})
\end{equation}

Assuming the real density for data $\{ x^{(i)}\}^m_{i=1}$ is $P_r$, the optimal $P_{\theta^\star}$ is equal to $P_r$ everywhere. It will minimize the Kullback-Leibler divergence $KL(P_r \parallel P_{\theta^\star})$.

\begin{equation}
KL(P_r \parallel P_{\theta}) = \int_x p_r(x) \log \frac{p_r(x)}{p_\theta(x)} dx
\end{equation}

Instead of directly finding $P_{\theta^\star}$, GAN uses a neural network $g_{\theta}$ with parameter $\theta$ that directly
generates samples following a certain distribution $P_{\theta}$. The input of $g_{\theta}$ is a random variable $Z$ with a fixed distribution $p(z)$. Through this approach, we can easily generate samples from a given distribution $p(z)$. $P_{r}$ can be represented by a low dimensional manifold. $z$ would have some semantic features.

<center>
    <img style="border-radius: 0.2em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 6px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/Kaimaoge/Kaimaoge.github.io/master/images/generator.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Figure １：DCGAN generator used in reference [1]. This structure projects a 100-dimensional vector obeying normal distribution to a realistic image. </div>
</center>

## Distance of traditional GAN

The GAN model architecture involves two sub-models: a generator model $G()$ for generating new examples and a discriminator model $D()$ for classifying whether generated examples are real, from the domain, or fake, generated by the generator model. 

<center>
    <img style="border-radius: 0.2em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 6px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/Kaimaoge/Kaimaoge.github.io/master/images/GAN.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Figure １：GAN framework. </div>
</center>

 We train $D()$ to maximize the probability of assigning the correct label (real:1, fake:0) to both training examples and samples from $G()$. This is equal to maximizing $E_{x \sim p_r} (\log D(x)) + E_{z \sim p_z} (\log (1 - D(G(z))))$. $G()$ is trained to make a fool of $D$. This is done by minimizing $E_{z \sim p_z} (\log (1 - D(G(z))))$. In other words,$D$ and $G$ play the following two-player minimax game
 
 \begin{equation}
\min_G \max_D L(G, D) = E_{x \sim p_r} (\log D(x)) + E_{z \sim p_z} (\log (1 - D(G(z)))).
\end{equation}

For fixed $G$, the optimal $D^\star$ is $\frac{p_r}{p_r + p_g}$. Minimizing $L(G, D^\star)$ is equal to minimizing Jensen–Shannon divergence between the $p_g$ and $p_r$ [2]. 

 \begin{equation}
JS(P_r \parallel P_g) = \frac{1}{2} KL(P_r \parallel \frac{P_g + P_r}{2}) + \frac{1}{2} KL(P_g \parallel \frac{P_g + P_r}{2}).
\end{equation}

## Wasserstein GAN [3]

KL and JS distances are measuring how close the model distribution $P_g$ and the real distribution $P_r$ are. The Wasserstein GAN [3] paper points out that the distances' impacts on the convergence of sequences of probability distributions. If the loss function defined upon the distances $d(P_g, P_r)$ is continuous, it is easier for GAN to converge.

[3] finds that Wasserstein distance $W(P_g, P_r)$ is continuous everywhere, and differentiable almost everywhere, if $g$ is continuous in $\theta$ and Lipschitz continuous. 

A real-valued function $f:R \to R$ is called $K$-Lipschitz continuous if there exists a real constant $K \geq 0$ such that, for all $x_1,x_2 \in R$,

\begin{equation}
|f(x_1) - f(x_2)| \leq K|x_1 - x_2|.
\end{equation}


 

